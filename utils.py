import os
import re
import glob
from pathlib import Path
import random
import numpy as np
import torch
import torch.nn as nn
from sklearn.metrics import f1_score, precision_score, recall_score

class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

def metrics(pred, true):
    pred = np.array(pred).reshape(-1)
    true = np.array(true).reshape(-1)
    # acc
    acc = np.mean((pred == true))
    # f_score
    f_score = f1_score(true, pred, average='macro')
    #precision
    precision = precision_score(true, pred, average='macro')
    #recall
    recall = recall_score(true, pred, average='macro')
    return acc, f_score, precision, recall

def increment_path(path, exist_ok=False, sep='', mkdir=False):
    # Increment file or directory path, i.e. runs/exp --> runs/exp{sep}2, runs/exp{sep}3, ... etc.
    path = Path(path)  # os-agnostic
    if path.exists() and not exist_ok:
        suffix = path.suffix
        path = path.with_suffix('')
        dirs = glob.glob(f"{path}{sep}*")  # similar paths
        matches = [re.search(rf"%s{sep}(\d+)" % path.stem, d) for d in dirs]
        i = [int(m.groups()[0]) for m in matches if m]  # indices
        n = max(i) + 1 if i else 2  # increment number
        path = Path(f"{path}{sep}{n}{suffix}")  # update path
    dir = path if path.suffix == '' else path.parent  # directory
    if not dir.exists() and mkdir:
        dir.mkdir(parents=True, exist_ok=True)  # make directory
    return path

import random
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score
import numpy as np
import torch
import pandas as pd

def F1(output, labels):
    output = output.argmax(1)
    output = output.cpu().detach().numpy()
    labels = labels.cpu().detach().numpy()
    micro = f1_score(labels, output,average='macro')
    return micro

def accuracy(output, labels):
    output = output.argmax(1)
    output = output.cpu().detach().numpy()
    labels = labels.cpu().detach().numpy()
    micro = accuracy_score(labels, output)
    return micro

def recall(output, labels):
    output = output.argmax(1)
    output = output.cpu().detach().numpy()
    labels = labels.cpu().detach().numpy()
    micro = recall_score(labels, output)
    return micro

def prec(output, labels):
    output = output.argmax(1)
    output = output.cpu().detach().numpy()
    labels = labels.cpu().detach().numpy()

    micro = precision_score(labels, output, average='macro')
    return micro

def get_performance(logits_q, y_qry):
    return F1(logits_q, y_qry), accuracy(logits_q, y_qry), recall(logits_q, y_qry), prec(logits_q, y_qry)